---
title: "APS Failure at Scania Trucks"
author: "Anastasiia Hryhorzhevska"
output:
  pdf_document: default
  html_document: default
---

**1. Proejct Goal**

Minimize maintenance costs of the air pressure system (APS) of Scania truck.

**2. Data **

The training and test sets are given :

 * Training set :  60'000 rows $\times$ 171 columns

 * Test set : 16'000 $\times$ 171

 * Target class : 1st column, "neg" - trucks with failures for components not related to the APS, "pos" - component failures for a specific component of the APS system 
 * Features : 170 numeric, 70 of which belong to 7 histograms with ten bins each

**3. Methodology**

* Missing values

* Impute missing values with median or another method could be used, e.g. EM

* Sum up all the values that belong to one histogram

* Outlier detection

* Feature significance and feature selection

* Dimensionality Reduction

* Undersampling to deal with imbalaned data

* Split training set into training and validaion sets

* Tune clasiification parameter

* Build model

* Evaluate the result. Falsely predicting a failure has a cost of 10, missing a failure a cost of 500, i.e. $f(X_n, X_p) = 10 \times X_n + 500 \times X_p$

---

```{r message=FALSE, warning=FALSE}
if(!require(dplyr)) {
  install.packages("dplyr"); require(dplyr)}

if(!require(ggplot2)) {
  install.packages("ggplot2"); require(ggplot2)}

if(!require(mice)) {
  install.packages("mice"); require(mice)}

if(!require(tidyr)) {
  install.packages("tidyr"); require(tidyr)}
```

```{r}
setwd("~/Git/aps-failure@st")
```

Load the data

```{r message = FALSE, warning = FALSE}
df      <- read.csv("training_set.csv", sep = ",") 
test.df <- read.csv("test_set.csv", sep = ",") 
```

```{r message = FALSE, warning = FALSE}
summary(df)
```

```{r}
table(df$class)
```
Split the data into set with neg samples and pos samples

```{r}
df.neg <- df[df$class == "neg", ]
df.pos <- df[df$class == "pos",]
```


**Missing value treatment**

Compute the percentage of missing values in each feature

```{r}
df[df == "na"] <- NA
nan.cnt.tbl    <- table(is.na(df[, -1]))
round(nan.cnt.tbl["TRUE"] / sum(nan.cnt.tbl) * 100, 2) # % of NaN value in the entire ds
```

```{r}

#' Count % of NaNs by column
#' @x input vector 
#' @return % of NaN at 'x'

CntNan <- function(x){
  tbl <- table(is.na(x))
  round(tbl["TRUE"] / sum(tbl) * 100, 2)
}

nans          <- sort(apply(df[, -1], 2, CntNan), decreasing = T)
var.extr.nans <- names(nans[nans > 20]) # take the names of vars that have more than 20% of nans and remove this vars from dataset, other nans will be imputed with mean
var.extr.nans
```

Exclude the varibales with more than 20% of NaNs from dataset

```{r message=F, warning=F}
df[, 1:ncol(df)] <- sapply(df[, 1:ncol(df)], as.numeric) # convert variables to numeric type
df$class         <- as.factor(df$class)
levels(df$class) <- c("neg", "pos")
df.red           <- select(df, -(var.extr.nans))
dim(df.red)
```

Impute the rest of NaNs with median of each variable. The reason why with median is because the data contains many outliers. It is obvious if we look at the basic statustics of teh data, e.g. the function $summary$ returns the Min, 1st Qu., Median, Mean 3rd Qu. and Max values.

```{r}
# imputes.mice <- mice(df.red, m = 1, maxit = 5, method = "rf") # another method that uses random forest but time costly: we will have 1 imputed datasets, every ds will be created after a maximum of 5 iterations

df.red[, 2:ncol(df.red)] <- sapply(df.red[, 2:ncol(df.red)], function(x) ifelse(is.na(x), 
                                                                median(x, na.rm = T),
                                                                x))

```

Now data has no missing values. Let's move further, and reduce the data by summing up the variables that represent each histogram, e.g. ag_hist = sum(ag_001 + ... + ag_009). 

```{r message = FALSE, warning = FALSE}
colnames(df.red)
```

```{r message = FALSE, warning = FALSE}
# ag_hist = sum(ag_000 + ... + ag_009)
# ay_hist = sum(ay_000 + ... + ay_009)) ... , az, ba, cn, cs, ee

df.red <- df.red %>% mutate(ag_hist = ag_000 + ag_001 + ag_002 + ag_003 + ag_004 + ag_005 + ag_006 + ag_007 + ag_008 + ag_009,
                              ay_hist = ay_000 + ay_001 + ay_002 + ay_003 + ay_004 + ay_005 + ay_006 + ay_007 + ay_008 + ay_009,
                              az_hist = az_000 + az_001 + az_002 + az_003 + az_004 + az_005 + az_006 + az_007 + az_008 + az_009,
                              ba_hist = ba_000 + ba_001 + ba_002 + ba_003 + ba_004 + ba_005 + ba_006 + ba_007 + ba_008 + ba_009,
                              cn_hist = cn_000 + cn_001 + cn_002 + cn_003 + cn_004 + cn_005 + cn_006 + cn_007 + cn_008 + cn_009,
                              cs_hist = cs_000 + cs_001 + cs_002 + cs_003 + cs_004 + cs_005 + cs_006 + cs_007 + cs_008 + cs_009,
                              ee_hist = ee_000 + ee_001 + ee_002 + ee_003 + ee_004 + ee_005 + ee_006 + ee_007 + ee_008 + ee_009) %>%
                      select(-c(ag_000 , ag_001 , ag_002 , ag_003 , ag_004 , ag_005 , ag_006 , ag_007 , ag_008 , ag_009,
                                ay_000 , ay_001 , ay_002 , ay_003 , ay_004 , ay_005 , ay_006 , ay_007 , ay_008 , ay_009,
                                az_000 , az_001 , az_002 , az_003 , az_004 , az_005 , az_006 , az_007 , az_008 , az_009,
                                ba_000 , ba_001 , ba_002 , ba_003 , ba_004 , ba_005 , ba_006 , ba_007 , ba_008 , ba_009,
                                cn_000 , cn_001 , cn_002 , cn_003 , cn_004 , cn_005 , cn_006 , cn_007 , cn_008 , cn_009,
                                cs_000 , cs_001 , cs_002 , cs_003 , cs_004 , cs_005 , cs_006 , cs_007 , cs_008 , cs_009,
                                ee_000 , ee_001 , ee_002 , ee_003 , ee_004 , ee_005 , ee_006 , ee_007 , ee_008 , ee_009))
```


**Outliers treatment**

1. PCA to builf clusters and identify the group with exreme value. During implementation memory issue appered, that's why another method was used.

2. Get rid out of outliers by computing the whiskers, i.e. 1.5 $\times$ IQR above and below 3rd and 1st quartiles, and exclude such observations from further analysis.

upper.whisker = $\min(\max(x), Q_3 + 1.5 * IQR)$
lower.whisker = $\max(\min(x), Q_1 - 1.5 * IQR)$

```{r message=F, warning=F}
bxplt         <- boxplot(df.red, plot = F)
upper.whisker <- bxplt$stats[5,]
lower.whisker <- bxplt$stats[1,]

df.outliers <- bxplt$out

FindOutliers <- function (col, up.wh, low.wh){
  val <- col[col < low.wh | col > up.wh]
  idx <- which(col %in% val)
  idx
}

outlier.idx <- vector()

for (c in 2:ncol(df.red))
  outlier.idx <- append(outlier.idx, FindOutliers(df.red[, c], upper.whisker[c], lower.whisker[c]))
outlier.idx <- unique(outlier.idx)

df.red.2 <- df.red[-outlier.idx,]
table(df.red.2$class)
```

Ooooo... there were 48'463 observations detected as outlliers, from which 995 belong to postive class. That's mean if we exclude all outliers from db, the df will consist of 11'533 negative observations and only 5 positive observations. This will make it impossible to build accurate model.

In other circumstances I would exclude some outliers, the most extreme, and replace the rest with the median or another metric or do somethng more what comes up during the implementation :), but here, since the task is about evaluating the candidate's line of thinking and basic code skills, I will split the data into two sets, one of which will have only negative observations and another - only positive. Then replace positive outlier values with the median of posititves and remove negative outliers from db. 

```{r}
df.neg <- df.red[df.red$class == "neg", ]
df.pos <- df.red[df.red$class == "pos",]
```

**Negative observations**

```{r message=F, warning=F}
bxplt         <- boxplot(df.neg, plot = F)
upper.whisker <- bxplt$stats[5,]
lower.whisker <- bxplt$stats[1,]

outlier.idx <- vector()

for (c in 2:ncol(df.neg))
  outlier.idx <- append(outlier.idx, FindOutliers(df.neg[, c], upper.whisker[c], lower.whisker[c]))
outlier.idx <- unique(outlier.idx)

df.neg.no.out <- df.neg[-outlier.idx,]
```

**Positive observations**

```{r message=F, warning=F}
bxplt         <- boxplot(df.pos, plot = F)
upper.whisker <- bxplt$stats[5,]
lower.whisker <- bxplt$stats[1,]

df.pos.no.out <- df.pos
for (c in 2:ncol(df.pos)){
  outlier.idx <- FindOutliers(df.pos[, c], upper.whisker[c], lower.whisker[c])
  df.pos.no.out[outlier.idx, c] <- median(df.pos.no.out[, c])
}
```

Combine positive and negative ds into one

```{r}
df.red <- rbind(df.neg.no.out, df.pos.no.out) 
dim(df.red)
```

```{r}
summary(df.red)
```

**Feature selection**

* Remove those variables mean and meadian of which are highly different 

```{r}
df.red <- df.red %>% select(-c(ai_000, aj_000, al_000, am_0, at_000, bc_000, cj_000, cl_000, cm_000, dx_000, dy_000))
summary(df.red)
```

* Remove all variables that contain the same val for each col, i.e. var(x) == 0, since they are not informative to distinguish two classes

```{r message=F, warning=F}
sum.each.col <- apply(df.red[, -1], 2, sum)
col.names    <- names(sum.each.col[sum.each.col == nrow(df.red)])
df.red       <- select(df.red, -col.names)
summary(df.red)
```

**Correlation between variables**

```{r message = FALSE, warning = FALSE}
if(!require(corrplot)) {
  install.packages("corrplot"); require(corrplot)}

if(!require(RColorBrewer)) {
  install.packages("RColorBrewer"); require(RColorBrewer)}

cor.mtrx <- cor(df.red[, -1], method = "pearson")

corrplot(cor.mtrx, type="upper", order="hclust", col = brewer.pal(n=8, name="RdYlBu"), tl.cex = 0.5)
```


It is seen that many of the variables have high correlation. Since we don't have the infromation about the contents of variables and their technical relationships, it is necessary to extract relevant variables using data analysis.

**Variance within variables. Distribution plots**

Let's have a look at the distribution of the reduced data. We want to have data that normally distributed.  Applying certain methods to data that are not normally distributed can give misleading or incorrect results. However, most methods that assume normality are robust enough for all data except the very abnormal. 

```{r}
df.red[, -1] %>%
  gather() %>%
    ggplot(aes(value)) +
      facet_wrap(~key, scales = "free")+
      geom_density()

```

O_o, only histogram variables follow normal (Gaussian) distribution. Two datasets for building classififcation models will be created. The first ds will contain only histogram variabes, the second ds - the most significant variables (the significance of vars will be identfired by either of three methods, PCA, linear regression, random forest).

```{r include=F}
# OR
# 
# FindOutliers <- function (col){
#   val <- boxplot(col, plot = F)$out
#   idx <- which(col %in% val)
#   idx
# }
# 
# for (c in 2:ncol(df.red))
#   outlier.idx <- append(outlier.idx, FindOutliers(df.red[, c]))
# outlier.idx <- unique(outlier.idx)

```

**Dimenationality reduction. PCA**

The reason to perform PCA:

- to find hidden patterns

- explore correlation between vars

- find significant vars

- prepare data for clustering to exclude outliers

```{r message = FALSE, warning = FALSE}
if(!require(FactoMineR)) {
  install.packages("FactoMineR"); require(FactoMineR)}

num.pc  <- 20 # number of principal components to compute
pca.res <- PCA(df.red, quali.sup = 1, scale = T, graph = F, ncp = num.pc)
```


```{r}
eigenvalues <- pca.res$eig
eigenvalues[1:20,]
```

First twenty PCs explain 70% of variation of entire ds. 

```{r message = FALSE, warning = FALSE}

if(!require(factoextra)){
  install.packages("factoextra"); require(factoextra)}

fviz_screeplot(pca.res, ncp = 20, linecolor = "red")
```

Following Kraiser rule, let's take the first 13 PC (eigenvalues of which are greater than 1) to further analysis.

```{r}
ncp <- 13
pca.df <- data.frame(pca.res$ind$coord[, 1:ncp])
```

**Variable factor map**

Variable factor map help to find the positively and negatively correlated vars, significance of each var (the closer the var is to circle of correlation, the more important in explaining the variability of ds), and together with individual factor map provide the infromation how the samples relate to each other.

```{r}
pca.res$var$contrib[, 1:10]
```

```{r message=F, warning=F}
fviz_pca_var(pca.res, col.var="contrib", title = "Variable factor map") +
  scale_color_gradient2(low = "blue", mid = "green", high = "red", midpoint = 3) + 
  theme_bw()
```

The frist two PCs explains almost 27% of variation of th entire ds. The most significant vars, according to PCA, bb_000, bu_000, bv_000 (red color on the plot).  moreover the correlation between them is equal to 1. This means that they carry the same information about variability in data, and two of these var could be excluded from the further analysis. The same situation is with bg_000 and ah_000. 

**Individuals factor map**

```{r message=F, warning=F}
fviz_pca_ind(pca.res, label="none", habillage=df.red$class, addEllipses = T, ellipse.level=0.95, title="Individual factor map")
```

Most of the two principal component axes are overlapped when plotted by class label, but the regions with a large number of positive classes and the regions with a dense and solid negative class are separated.

**Feature significance**

Let's build few models to identify the most significant vars.

* Linear Regression Model

```{r message = FALSE, warning = FALSE}
df.test <- df.red

if(!require(relaimpo)){
  install.packages("relaimpo"); require(relaimpo)}

if(!require(caret)){
  install.packages("caret"); require(caret)}


lm.model      <- lm( class ~. , data = data.frame(cbind(class=df.test$class, scale(df.test[, -1], center = T, scale = T)))) # fit lm() model
lm.importance <- caret::varImp(lm.model)[1]
lm.importance <- data.frame(cbind(Variable = rownames(lm.importance), Importance = lm.importance$Overall))
lm.importance$Importance <- as.numeric(as.character(lm.importance$Importance))
lm.importance <- lm.importance[order(lm.importance$Importance, decreasing = T), ][1:20,]
lm.importance
```

```{r message = FALSE, warning = FALSE}
ggplot(data = lm.importance, aes(x =Variable, y = Importance, group=1)) +
  geom_line(linetype = "dashed") +
  geom_point() + 
  ggtitle("Linear Regression Model") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=0.5))
```

* Random Forest

```{r message = FALSE, warning = FALSE}
if(!require(caret)){
  install.packages("caret"); require(caret)}

if(!require(randomForest)){
  install.packages("randomForest"); require(randomForest)}

random.forest.mdl <- randomForest(class ~., data = df.red, importance = T)

# Overall importance, i.e. Overall class = pos class + neg class
import <- caret::varImp(random.forest.mdl)[1]
rf.sign.overall <- data.frame(cbind(Variable = rownames(import), Importance = import, Class = "Overall"))
colnames(rf.sign.overall) <- c("Variable", "Importance", "Class")

# Negative class importance
rf.sign.neg <- data.frame(cbind(Variable = rownames(import), Importance = importance(random.forest.mdl, scale = T)[, 1], Class = "neg"))

# Posititve class importance
rf.sign.pos <-data.frame(cbind(Variable = rownames(import), Importance = importance(random.forest.mdl, scale = T)[, 2], Class = "pos"))

rf.sign <- rbind(rf.sign.overall, rf.sign.neg, rf.sign.pos)
rf.sign$Variable <- (rf.sign$Variable)
rf.sign$Importance <- as.numeric(rf.sign$Importance)
```

```{r message = FALSE, warning = FALSE}
ggplot(data = rf.sign[], aes(x = Variable, y = Importance, group = Class, color = Class)) +
  geom_line(linetype = "dashed") +
  geom_point() + 
  ggtitle("Random Forest Model") +
  theme(legend.position = "bottom",  axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=0.5))
```

For further analysis ten most significant var according to RF algorithm was selected:

```{r}
rf.most.sign <- as.character(rf.sign.overall[order(rf.sign.overall$Importance, decreasing = T), ][1:10, "Variable"])
df.rf.red   <- cbind(class = df.red$class, data.frame(scale(df.red[, rf.most.sign], center = T, scale = T)))
```

**Undersampling and classification**

Since the data are highly imbalanced, complement using the oversampling method or the undersampling method, and balance each class to create a model. Here, I will use only undersampling.

Methodology :

* Randomly sample 1000 observations from negative class and combine them with positive observations. This action will be repeated $m$ times and next steps will be applied for the new sampled subset;

* The new created data set will be  divided into training and validation, 75% and 25%;

* The cost, c, parameter in SVM will be tuned;

* Each experiment will repeated n times and average of evaluation metrics were computed;

* Confusion matrices will built and evaluation metrics will computed for both training and validation data sets.

```{r}
df.mdl <- df.rf.red
```

Some functions for classififcation

* Function for evaluation

```{r}
#' Evaluation the classification model results
#' @param conf.matr confusion matrix
#' @param data.type if "train", training data, if "test", test data
#' @return df with evaluation measures

EvalClassifier <- function(conf.matr, data.type = "train"){
  measures           <- data.frame(matrix(nrow = 1, ncol = 6))
  colnames(measures) <- c("Cost", "Error", "Accuracy", "Recall", "Precision", "F1")
  
  measures[1, "Cost"]          <- 10 * conf.matr[2, 1] + 500 * conf.matr[1, 2]
  measures[1, "Accuracy"]      <- (sum(conf.matr[row(conf.matr) == col(conf.matr)]) / sum(conf.matr)) * 100
  measures[1, "Error"]         <- 100 - measures[1, "Accuracy"]
  measures[1, "Recall"]        <- (conf.matr[2, 2] / sum(conf.matr[, 2])) * 100
  measures[1, "Precision"]     <- (conf.matr[2, 2] / sum(conf.matr[2, ])) * 100
  measures[1, "F1"]            <- mean(2 * measures[1, "Precision"] * measures[1, "Recall"] / (measures[1, "Precision"] + measures[1, "Recall"]))
  
  measures                     <- round(measures, 1)
  measures                     <- cbind(Data = data.type, measures[, -3])
  
  return(measures)
}
```

**SVM classifier**

Here let's build only SVM with linear kernel.

* Function for tuning cost, *c*,  parameter in SVM classifier 

```{r message = FALSE, warning = FALSE}

if(!require(kernlab)) {
  install.packages("kernlab"); require(kernlab)}

#' Tuning cost param, c,  parameter in SVM classifier 
#' @param input.data training dataset with target variable that should be named as 'class'
#' @return df with cost param, number of support vactors and calculated errors for each *c*

TuneCinSVM <- function (input.data)
{
  cost.params    <- 10^(-2:1)
  errs           <- data.frame(cbind(C = cost.params, Error = vector("numeric", length(cost.params)), nSV = vector("numeric", length(cost.params))))
  
  for (c in cost.params){
      model <-  ksvm (class ~ ., data = input.data, kernel = 'polydot', C = c, cross = nrow(input.data))
      errs[errs$C == c, "Error"] <- cross(model) * 100
      errs[errs$C == c, "nSV"]   <- nSV(model) * 100
    }
  
  return(errs)
}

```

* Function for SVM classifier.

```{r message = FALSE, warning = FALSE}
if(!require(e1071)) {
  install.packages("e1071"); require(e1071)}


#' @param train.df training data, 'class' is a target var and placed in the first column
#' @param train.df test data, 'class' is a target var and placed in the first column
#' @param best.c best 'c' value obtaied by TuneCinSVM function
#' @return df with evaluation measures for training and test sets, function EvalClassifier is used

SVMClassifier <- function(train.df, test.df, best.c){
  svm.mdl          <- svm (train.df[, -1], train.df$class, type = "C-classification", kernel = 'linear', C = best.c, cross = nrow(train.df))
  prediction.train <- predict(svm.mdl, train.df[, -1])
  conf.matr        <- table(prediction.train, train.df$class)
  eval.tbl.train   <- EvalClassifier(conf.matr, "train")
  
  prediction.test  <- predict(svm.mdl, test.df[, -1])
  conf.matr        <- table(prediction.test, test.df$class)
  eval.tbl.test    <- EvalClassifier(conf.matr, "test")
  
  eval.tbl.svm     <- rbind(eval.tbl.train, eval.tbl.test)
  
  return(list(mdl = svm.mdl, metrics = eval.tbl.svm, conf.matr = conf.matr) )
}
```

```{r}
set.seed(123)

df.neg <- df.mdl[df.mdl$class == "neg", ]
df.pos <- df.mdl[df.mdl$class == "pos",]

neg.smpl.size  <- 1000
sampling.n.rep <- 1#round(nrow(df.neg) / neg.smpl.size, 0)

eval.tbl.svm.final           <-  data.frame(matrix(0, nrow = 2, ncol = 5))
colnames(eval.tbl.svm.final) <- c("Cost", "Error", "Recall", "Precision", "F1")

for (j in 1:sampling.n.rep){
  
  neg.ind     <- sample(seq_len(nrow(df.neg)), size = neg.smpl.size) # sample 1000 observations from negative class
  df.neg.smpl <- df.neg[neg.ind,]
  df.smpl     <- rbind(df.pos, df.neg.smpl)

  smp.size <- floor(0.75 * nrow(df.smpl))
  train.ind <- sample(seq_len(nrow(df.smpl)), size = smp.size)

  train.df <- df.smpl[train.ind, ]
  valid.df  <- df.smpl[-train.ind, ] # validation ds

  # rbind(train = table(train.df$class), test = table(valid.df$class))

  svm.c <- TuneCinSVM(train.df) # Tune the 'c' parameter in SVM

  # Plots of obtained cost parameters and number of support vectors SV). The best cost parameter is selected based on the lowest error and smallest number of SV.

  ggplot(data = svm.c, aes(x = C, y = Error, group = 1)) +
    geom_line(linetype = "dashed")+
    geom_point() + 
    ggtitle("Error")

  ggplot(data = svm.c, aes(x = C, y = nSV, group = 1)) +
    geom_line(linetype = "dashed", color = "red") +
    geom_point() + 
    ggtitle("Number of Support Vectors")

  # Build the model and evaluate the result

  best.c <- svm.c[svm.c$Error == min(svm.c$Error), ] # the lowest error is recieved 
  best.c <- best.c[best.c$nSV == min(best.c$nSV), "C"]

  svm.mdl <-  SVMClassifier(train.df, valid.df, best.c)
  eval.tbl.svm <- svm.mdl$metrics[, -1]
  mdl <- svm.mdl$mdl
  conf.matr <- svm.mdl$conf.matr
  
  n.rep <- 2
  for (i in 1:(n.rep - 1)){
    train.ind  <- sample(seq_len(nrow(df.smpl)), size = smp.size)
    train.df.2 <- df.smpl[train.ind, ]
    valid.df.2 <- df.smpl[-train.ind, ]
  
    eval.tbl.svm <- eval.tbl.svm + SVMClassifier(train.df.2, valid.df.2, best.c)$metrics[, -1]
  }
  eval.tbl.svm         <- eval.tbl.svm / n.rep
  eval.tbl.svm.final   <- eval.tbl.svm.final + eval.tbl.svm
}
  eval.tbl.svm.final   <- eval.tbl.svm.final / sampling.n.rep
  eval.tbl.svm.final   <- cbind(Data=c("train", "valid"), eval.tbl.svm)
  eval.tbl.svm.final
```

**Prediction for test data**

First we must prepare data, i.e. transform to the same view as training set, and deal with missing values and outliers. 
```{r}
summary(test.df)
```

Prepare test ds

```{r message=F, warning= F}
test.df[test.df == "na"] <- NA

test.ids                   <- test.df$id
test.df[, 3:ncol(test.df)] <- sapply(test.df[, 3:ncol(test.df)], as.numeric) # convert variables to numeric type
test.red.df                <- test.df[, -1] # remove column id from ds

# 1. Missing values : impute with median
test.red.df[, 2:ncol(test.red.df)] <- sapply(test.red.df[, 2:ncol(test.red.df)], function(x) ifelse(is.na(x), 
                                                                median(x, na.rm = T),
                                                                x))
# 2. Summ up the variables that represent each histogram, e.g. ag_hist = sum(ag_001 + ... + ag_009), and select only those variables which RF model predict asthe most significant

test.red.df <- test.red.df %>% mutate(ag_hist = ag_000 + ag_001 + ag_002 + ag_003 + ag_004 + ag_005 + ag_006 + ag_007 + ag_008 + ag_009,
                              ay_hist = ay_000 + ay_001 + ay_002 + ay_003 + ay_004 + ay_005 + ay_006 + ay_007 + ay_008 + ay_009,
                              az_hist = az_000 + az_001 + az_002 + az_003 + az_004 + az_005 + az_006 + az_007 + az_008 + az_009,
                              ba_hist = ba_000 + ba_001 + ba_002 + ba_003 + ba_004 + ba_005 + ba_006 + ba_007 + ba_008 + ba_009,
                              cn_hist = cn_000 + cn_001 + cn_002 + cn_003 + cn_004 + cn_005 + cn_006 + cn_007 + cn_008 + cn_009,
                              cs_hist = cs_000 + cs_001 + cs_002 + cs_003 + cs_004 + cs_005 + cs_006 + cs_007 + cs_008 + cs_009,
                              ee_hist = ee_000 + ee_001 + ee_002 + ee_003 + ee_004 + ee_005 + ee_006 + ee_007 + ee_008 + ee_009) 

test.red.df <- test.red.df[, rf.most.sign]

# 3. Outliers treating. Replace all outliers with median value of each column

bxplt         <- boxplot(test.red.df, plot = F)
upper.whisker <- bxplt$stats[5,]
lower.whisker <- bxplt$stats[1,]

for (c in 1:ncol(test.red.df)){
  outlier.idx <- FindOutliers(test.red.df[, c], upper.whisker[c], lower.whisker[c])
  test.red.df[outlier.idx, c] <- median(test.red.df[, c])
}

#4. Normalize data

test.red.df.norm <- data.frame(scale(test.red.df, center = T, scale = T))
```

```{r}
test.red.df.norm %>%
  gather() %>%
    ggplot(aes(value)) +
      facet_wrap(~key, scales = "free")+
      geom_density()

```

```{r}
prediction.test <- predict(mdl, test.red.df.norm)
write.table(data.frame(prediction.test), "~/Git/aps-failure@st/aps_prediction.csv", append = FALSE, sep = ",", row.names = T, col.names = F)
```

```{sh include = F}
ls -lh ~/Git/aps-failure@st
```
