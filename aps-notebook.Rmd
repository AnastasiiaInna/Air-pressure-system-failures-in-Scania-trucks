---
title: "APS Failure at Scania Trucks"
author: Anastasiia Hryhorzhevska
output: 
  html_document: default
---

**1. Proejct Goal**

Minimize maintenance costs of the air pressure system (APS) of Scania truck.

**2. Data **

The training and test sets are given :

 * Training set :  60'000 rows $\times$ 171 columns

 * Test set : 16'000 $\times$ 171

 * Target class : 1st column, "neg" - trucks with failures for components not related to the APS, "pos" - component failures for a specific component of the APS system 

 * Features : 170 numeric, 70 of which belong to 7 histograms with ten bins each

**3. Methodology**

* Missing values

* Impute missing values with mean or median or another method could be used, e.g. EM

* Outlier detection

* Sum up all the values that belong to one histogram

* Feature significance and feature selection

* Dimensionality Reduction

* Oversampling to deal with imbalaned data

* Split training set into training and validaion sets

* Tune clasiification parameter

* Build model

* Evaluate the result. Falsely predicting a failure has a cost of 10, missing a failure a cost of 500, i.e. $f(X_n, X_p) = 10 \times X_n + 500 \times X_p$

---

```{r message=FALSE, warning=FALSE}
if(!require(dplyr)) {
  install.packages("dplyr"); require(dplyr)}

if(!require(ggplot2)) {
  install.packages("ggplot2"); require(ggplot2)}

if(!require(mice)) {
  install.packages("mice"); require(mice)}

if(!require(tidyr)) {
  install.packages("tidyr"); require(tidyr)}
```

```{r}
setwd("~/Git/aps-failure@st")
```

Load the data

```{r message = FALSE, warning = FALSE}
df      <- read.csv("training_set.csv", sep = ",") 
test.df <- read.csv("test_set.csv", sep = ",") 
```

```{r message = FALSE, warning = FALSE}
summary(df)
```

```{r}
table(df$class)
```
Split the data into set with neg samples and pos samples

```{r}
df.neg <- df[df$class == "neg", ]
df.pos <- df[df$class == "pos",]
```

Compute the percentage of missing values in each feature

```{r}
df[df == "na"] <- NA
nan.cnt.tbl    <- table(is.na(df[, -1]))
round(nan.cnt.tbl["TRUE"] / sum(nan.cnt.tbl) * 100, 2) # % of NaN value in the entire ds
```

```{r}
CntNan <- function(x){
  tbl <- table(is.na(x))
  round(tbl["TRUE"] / sum(tbl) * 100, 2)
}

nans          <- sort(apply(df[, -1], 2, CntNan), decreasing = T)
var.extr.nans <- names(nans[nans > 20]) # take the names of vars that have more than 20% of nans and remove this vars from dataset, other nans will be imputed with mean
var.extr.nans
```

Exclude the varibales with more than 20% of NaNs from dataset

```{r}
df[, 1:ncol(df)] <- sapply(df[, 1:ncol(df)], as.numeric) # convert variables to numeric type
df$class         <- as.factor(df$class)
levels(df$class) <- c("neg", "pos")
df.red           <- select(df, -(var.extr.nans))
dim(df.red)
```

Impute the rest of NaNs with median of each variable. The reason why with median is because the data contains many outliers. It is obvious if we look at the basic statustics of teh data, e.g. the function $summary$ returns the Min, 1st Qu., Median, Mean 3rd Qu. and Max values.

```{r}
imputes.mice <- mice(df.red, m = 1, maxit = 5, method = "rf") # another method that uses random forest but time costly: we will have 1 imputed datasets, every ds will be created after a maximum of 5 iterations

df.red[, 2:ncol(df.red)] <- sapply(df.red[, 2:ncol(df.red)], function(x) ifelse(is.na(x), 
                                                                median(x, na.rm = T),
                                                                x))

```

Now data has no missing values. Let's move further, and reduce the data by summing up the variables that represent each histogram, e.g. ag_hist = sum(ag_001 + ... + ag_009). 

```{r message = FALSE, warning = FALSE}
colnames(df.red)
```

```{r message = FALSE, warning = FALSE}
# ag_hist = sum(ag_000 + ... + ag_009)
# ay_hist = sum(ay_000 + ... + ay_009)) ... , az, ba, cn, cs, ee

df.red.2 <- df.red %>% mutate(ag_hist = ag_000 + ag_001 + ag_002 + ag_003 + ag_004 + ag_005 + ag_006 + ag_007 + ag_008 + ag_009,
                              ay_hist = ay_000 + ay_001 + ay_002 + ay_003 + ay_004 + ay_005 + ay_006 + ay_007 + ay_008 + ay_009,
                              az_hist = az_000 + az_001 + az_002 + az_003 + az_004 + az_005 + az_006 + az_007 + az_008 + az_009,
                              ba_hist = ba_000 + ba_001 + ba_002 + ba_003 + ba_004 + ba_005 + ba_006 + ba_007 + ba_008 + ba_009,
                              cn_hist = cn_000 + cn_001 + cn_002 + cn_003 + cn_004 + cn_005 + cn_006 + cn_007 + cn_008 + cn_009,
                              cs_hist = cs_000 + cs_001 + cs_002 + cs_003 + cs_004 + cs_005 + cs_006 + cs_007 + cs_008 + cs_009,
                              ee_hist = ee_000 + ee_001 + ee_002 + ee_003 + ee_004 + ee_005 + ee_006 + ee_007 + ee_008 + ee_009) %>%
                      select(-c(ag_000 , ag_001 , ag_002 , ag_003 , ag_004 , ag_005 , ag_006 , ag_007 , ag_008 , ag_009,
                                ay_000 , ay_001 , ay_002 , ay_003 , ay_004 , ay_005 , ay_006 , ay_007 , ay_008 , ay_009,
                                az_000 , az_001 , az_002 , az_003 , az_004 , az_005 , az_006 , az_007 , az_008 , az_009,
                                ba_000 , ba_001 , ba_002 , ba_003 , ba_004 , ba_005 , ba_006 , ba_007 , ba_008 , ba_009,
                                cn_000 , cn_001 , cn_002 , cn_003 , cn_004 , cn_005 , cn_006 , cn_007 , cn_008 , cn_009,
                                cs_000 , cs_001 , cs_002 , cs_003 , cs_004 , cs_005 , cs_006 , cs_007 , cs_008 , cs_009,
                                ee_000 , ee_001 , ee_002 , ee_003 , ee_004 , ee_005 , ee_006 , ee_007 , ee_008 , ee_009))
```

PCA

- to find hidden patterns

- explore correlation between vars

- find significant vars

- prepare data for clustering to exclude outliers

```{r message = FALSE, warning = FALSE}
if(!require(FactoMineR)) {
  install.packages("FactoMineR"); require(FactoMineR)}

num.pc  <- 20
pca.res <- PCA(df.red.2, quali.sup = 1, scale = T, graph = F, ncp = num.pc)
```


```{r}
eigenvalues <- pca.res$eig
eigenvalues
```

```{r message = FALSE, warning = FALSE}

if(!require(factoextra)){
  install.packages("factoextra"); require(factoextra)}

fviz_screeplot(pca.res, ncp = 20, linecolor = "red")
```

Let's take the first 5 PC to cluster the data and evaluate outliers to be removed. Once it is done, we will come back to PCA more precisely

```{r}
ncp <- 5
pca.df <- data.frame(pca.res$ind$coord[, 1:ncp])

distM <- dist(pca.df, method = "euclidean")
            hc <- hclust(distM, method = "ward.D")
            hc.groups <- cutree(hc, k)
```

**Variable factor map**

```{r}
pca.res$var$contrib[1:10, 1:10]
```

```{r}
fviz_pca_var(pca.res, col.var="contrib") +
  scale_color_gradient2(low = "blue", mid = "green", high = "red", midpoint = 4) + 
  theme_bw()
```

```{r}
ggplot(data = nans, aes(x = Variable, y = nans, color = "blue")) +
  geom_line(linetype = "dashed") +
  geom_point() + 
  ggtitle("Missing values detection") +
  xlab("% of NaNs") +
  ylab("Variables")
```
